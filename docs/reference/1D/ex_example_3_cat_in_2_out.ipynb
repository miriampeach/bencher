{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9404c9f9",
   "metadata": {},
   "source": [
    "# example_3_cat_in_2_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5236f083",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\"\"\"This file demonstrates benchmarking with 3 categorical inputs and 2 output variables.\n",
    "\n",
    "It benchmarks different Python operations to compare their performance characteristics\n",
    "using simulated performance data to illustrate how benchmarking works.\n",
    "\"\"\"\n",
    "\n",
    "import random\n",
    "import bencher as bch\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "\n",
    "class PythonOperationsBenchmark(bch.ParametrizedSweep):\n",
    "    \"\"\"Example class for benchmarking different Python operations using categorical variables.\n",
    "\n",
    "    This class demonstrates how to structure a benchmark with multiple input parameters\n",
    "    and multiple output metrics. It uses simulated performance data that follows realistic\n",
    "    patterns while being deterministic and reproducible.\n",
    "    \"\"\"\n",
    "\n",
    "    data_structure = bch.StringSweep([\"list\", \"dict\"], doc=\"Type of data structure to operate on\")\n",
    "    operation_type = bch.StringSweep([\"read\", \"write\"], doc=\"Type of operation to perform\")\n",
    "    data_size = bch.StringSweep([\"small\", \"medium\"], doc=\"Size of data to process\")\n",
    "\n",
    "    execution_time = bch.ResultVar(units=\"ms\", doc=\"Execution time in milliseconds\")\n",
    "    memory_peak = bch.ResultVar(units=\"KB\", doc=\"Peak memory usage in kilobytes\")\n",
    "\n",
    "    def __call__(self, **kwargs) -> dict:\n",
    "        \"\"\"Execute the benchmark for the given set of parameters.\n",
    "\n",
    "        Args:\n",
    "            **kwargs: Parameters to update before executing\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing the benchmark results\n",
    "        \"\"\"\n",
    "        self.update_params_from_kwargs(**kwargs)\n",
    "\n",
    "        # Use deterministic fake data based on parameters\n",
    "        # Base values that will be modified by our parameters\n",
    "        base_time = 10.0  # ms\n",
    "        base_memory = 100.0  # KB\n",
    "\n",
    "        # Adjust for data structure (lists are generally faster but use more memory)\n",
    "        if self.data_structure == \"list\":\n",
    "            time_factor = 0.8\n",
    "            memory_factor = 1.2\n",
    "        else:  # dict\n",
    "            time_factor = 1.2\n",
    "            memory_factor = 0.9\n",
    "\n",
    "        # Adjust for operation type (reads are faster than writes)\n",
    "        if self.operation_type == \"read\":\n",
    "            time_factor *= 0.7\n",
    "            memory_factor *= 0.8\n",
    "        else:  # write\n",
    "            time_factor *= 1.4\n",
    "            memory_factor *= 1.3\n",
    "\n",
    "        # Adjust for data size\n",
    "        if self.data_size == \"medium\":\n",
    "            time_factor *= 5\n",
    "            memory_factor *= 10\n",
    "\n",
    "        # Calculate final metrics with increased variance\n",
    "        self.execution_time = base_time * time_factor * random.uniform(0.85, 1.15)\n",
    "        self.memory_peak = base_memory * memory_factor * random.uniform(0.90, 1.10)\n",
    "\n",
    "        return super().__call__(**kwargs)\n",
    "\n",
    "\n",
    "def example_3_cat_in_2_out(\n",
    "    run_cfg: bch.BenchRunCfg = None, report: bch.BenchReport = None\n",
    ") -> bch.Bench:\n",
    "    \"\"\"This example demonstrates benchmarking with categorical variables and multiple output metrics.\n",
    "\n",
    "    It creates a synthetic benchmark that simulates performance characteristics of different\n",
    "    Python operations, varying data structures, operation types, and data sizes. The benchmark\n",
    "    produces realistic patterns of execution time and memory usage without actually executing\n",
    "    real operations, making it ideal for learning and demonstration.\n",
    "\n",
    "    Args:\n",
    "        run_cfg: Configuration for the benchmark run\n",
    "        report: Report to append the results to\n",
    "\n",
    "    Returns:\n",
    "        bch.Bench: The benchmark object\n",
    "    \"\"\"\n",
    "\n",
    "    if run_cfg is None:\n",
    "        run_cfg = bch.BenchRunCfg()\n",
    "    run_cfg.repeats = 5  # Fewer repeats for a quicker benchmark\n",
    "    bench = PythonOperationsBenchmark().to_bench(run_cfg, report)\n",
    "    bench.plot_sweep(\n",
    "        title=\"Python Operations Performance Benchmark\",\n",
    "        description=\"Comparing execution time and peak memory usage across Python data structures and operations\",\n",
    "        post_description=\"\"\"\n",
    "        This benchmark illustrates how different data structures and operations affect performance.\n",
    "        \n",
    "        Key observations:\n",
    "        - Lists generally process faster than dictionaries for these operations\n",
    "        - Read operations outperform write operations as expected\n",
    "        - Medium-sized data requires significantly more resources than small data\n",
    "        - Note that variance in the results simulates real-world measurement fluctuations\n",
    "        \"\"\",\n",
    "    )\n",
    "    return bench\n",
    "\n",
    "\n",
    "bench = example_3_cat_in_2_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39eba6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.io import output_notebook\n",
    "\n",
    "output_notebook()\n",
    "bench.get_result().to_auto_plots()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
